{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "spanish-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "radio-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../dataGeneration/problemData.csv\"                                                                                                                                                                                            \n",
    "data = pd.read_csv(file_path)\n",
    "NON_HINT_TAGS = {'array', 'string', 'math', 'tree', 'graph', 'design', 'brainteaser', 'linked-list', 'geometry', 'random'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "returning-vaccine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947 947\n",
      "You are given n​​​​​​ tasks labeled from 0 to n - 1 represented by a 2D integer array tasks, where tasks[i] = [enqueueTimei, processingTimei] means that the i​​​​​​th​​​​ task will be available to process at enqueueTimei and will take processingTimei to finish processing.\n",
      "\n",
      "You have a single-threaded CPU that can process at most one task at a time and will act in the following way:\n",
      "\n",
      "\n",
      "\tIf the CPU is idle and there are no available tasks to process, the CPU remains idle.\n",
      "\tIf the CPU is idle and there are available tasks, the CPU will choose the one with the shortest processing time. If multiple tasks have the same shortest processing time, it will choose the task with the smallest index.\n",
      "\tOnce a task is started, the CPU will process the entire task without stopping.\n",
      "\tThe CPU can finish a task then start a new one instantly.\n",
      "\n",
      "\n",
      "Return the order in which the CPU will process the tasks. ['heap']\n"
     ]
    }
   ],
   "source": [
    "problemDescs = []\n",
    "problemTags = []\n",
    "\n",
    "for i, d in data.iterrows():\n",
    "    tags = [t for t in ast.literal_eval(d['tags']) if t not in NON_HINT_TAGS]\n",
    "    \n",
    "    if tags:\n",
    "        problemDescs.append(d['description'])\n",
    "        problemTags.append(tags)\n",
    "        \n",
    "print(len(problemDescs), len(problemTags))\n",
    "print((problemDescs[0]), (problemTags[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "typical-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexpTokenizer = RegexpTokenizer(\"[a-zA-Z]{2,}\")\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(doc):\n",
    "    return [wnl.lemmatize(t) for t in regexpTokenizer.tokenize(doc)]\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english', max_df=0.7, min_df=5)\n",
    "X = vectorizer.fit_transform(problemDescs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "canadian-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "hintTags = set()\n",
    "\n",
    "for tags in problemTags:\n",
    "    for t in tags:\n",
    "        hintTags.add(t)\n",
    "        \n",
    "hintTags = np.array(list(hintTags))\n",
    "tagToIndex = {tag: i for i, tag in enumerate(hintTags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "handmade-dream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = []\n",
    "\n",
    "for tags in problemTags:\n",
    "    y = [0] * len(hintTags)\n",
    "    \n",
    "    for t in tags:\n",
    "        y[tagToIndex[t]] = 1\n",
    "        \n",
    "    Y.append(y)\n",
    "    \n",
    "Y = np.array(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "municipal-uruguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(947, 649) (947, 32)\n",
      "(710, 649) (237, 649) (710, 32) (237, 32)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=1)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aging-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network. Not used.\n",
    "regr = MLPRegressor((500, 200, 500), max_iter=100).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "assumed-fisher",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-168-dfbc9d0144db>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-168-dfbc9d0144db>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    NN:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "NN:\n",
    "100:\n",
    "['dynamic-programming',\n",
    " 'depth-first-search',\n",
    " 'breadth-first-search',\n",
    " 'segment-tree',\n",
    " 'divide-and-conquer']\n",
    "500:\n",
    "['depth-first-search',\n",
    " 'binary-search',\n",
    " 'segment-tree',\n",
    " 'breadth-first-search',\n",
    " 'binary-indexed-tree'] \n",
    "\n",
    "NB:\n",
    "\"Count the number of islands\"\n",
    "Fit_prior:\n",
    "    True:\n",
    "        ['depth-first-search' 'dynamic-programming' 'breadth-first-search'\n",
    " 'greedy' 'binary-search']\n",
    "    False:\n",
    "        ['depth-first-search' 'breadth-first-search' 'dynamic-programming'\n",
    "         'binary-search' 'backtracking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "decimal-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutinomial Naive Bayes. Used in app.\n",
    "clf = MultiOutputClassifier(MultinomialNB(fit_prior=False), n_jobs=1).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "korean-highlight",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6492617909729264, -0.44858443001798043, -0.36961440800303363, -0.3558985033686547, -0.3374208078543204]\n",
      "['binary-search' 'hash-table' 'sort' 'bit-manipulation' 'greedy']\n"
     ]
    }
   ],
   "source": [
    "def classify(classifier, text):\n",
    "    x = vectorizer.transform([text])\n",
    "    class_probs = np.array([p[0][1] for p in classifier.predict_proba(x)])\n",
    "    print(sorted(-class_probs)[:5])\n",
    "    return hintTags[(-class_probs).argsort()[:5]]\n",
    "\n",
    "print(classify(clf, \"Find the number in a sorted array that equal to the index the element is at, and requires log(n) complexity (find any such element is ok)\"))\n",
    "#print(classify(clf, \"randomly choosing k samples from a list of n items, where n is either a very large or unknown number. Typically n is large enough that the list doesn’t fit into main memory. For example, a list of search queries in Google and Facebook\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "speaking-brunei",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hash-table', 'trie', 'sliding-window', 'binary-search-tree',\n",
       "       'union-find', 'greedy', 'rolling-hash', 'recursion',\n",
       "       'divide-and-conquer', 'minimax', 'binary-indexed-tree',\n",
       "       'rejection-sampling', 'dequeue', 'queue', 'bit-manipulation',\n",
       "       'line-sweep', 'suffix-array', 'backtracking', 'heap',\n",
       "       'segment-tree', 'binary-search', 'sort', 'two-pointers',\n",
       "       'ordered-map', 'memoization', 'dynamic-programming',\n",
       "       'meet-in-the-middle', 'breadth-first-search', 'reservoir-sampling',\n",
       "       'topological-sort', 'stack', 'depth-first-search'], dtype='<U20')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hintTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-passport",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
